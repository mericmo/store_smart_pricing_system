import optuna
from sklearn.model_selection import TimeSeriesSplit, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
warnings.filterwarnings('ignore')


class ModelOptimizer:
    """æ¨¡å‹å‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(self, trainer_instance):
        """
        åˆå§‹åŒ–æ¨¡å‹ä¼˜åŒ–å™¨

        Args:
            trainer_instance: SalesPredictorå®ä¾‹
        """
        self.trainer = trainer_instance
        self.best_params = {}
        self.optimization_results = {}

    def prepare_cv_data(self, features_df, target_col='é”€å”®æ•°é‡'):
        """
        å‡†å¤‡äº¤å‰éªŒè¯æ•°æ®

        Returns:
            X, y, feature_columns
        """
        X_train, X_test, y_train, y_test, feature_columns = self.trainer._get_features_for_training(
            features_df, target_col
        )

        # åˆå¹¶æ•°æ®ç”¨äºäº¤å‰éªŒè¯
        X = pd.concat([X_train, X_test])
        y = pd.concat([y_train, y_test])

        return X, y, feature_columns

    def time_series_cv_split(self, X, y, n_splits=5):
        """
        æ—¶é—´åºåˆ—äº¤å‰éªŒè¯åˆ†å‰²

        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
            n_splits: åˆ†å‰²æ•°

        Yields:
            è®­ç»ƒé›†å’ŒéªŒè¯é›†ç´¢å¼•
        """
        n_samples = len(X)
        fold_size = n_samples // n_splits

        for i in range(n_splits - 1):
            train_end = (i + 1) * fold_size
            val_start = train_end
            val_end = val_start + fold_size

            train_indices = list(range(0, train_end))
            val_indices = list(range(val_start, min(val_end, n_samples)))

            yield train_indices, val_indices

    def optimize_lightgbm(self, features_df, target_col='é”€å”®æ•°é‡',
                          n_trials=50, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–LightGBMå‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            params = {
                'objective': 'regression',
                'metric': 'rmse',
                'verbose': -1,
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),
                'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),
                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),
                'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
                'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                'max_depth': trial.suggest_int('max_depth', -1, 50),
                'min_split_gain': trial.suggest_float('min_split_gain', 0, 10),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'num_boost_round': trial.suggest_int('num_boost_round', 100, 2000),
            }

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                train_data = lgb.Dataset(X_train_fold, label=y_train_fold)
                val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)

                model = lgb.train(
                    params,
                    train_data,
                    num_boost_round=params.pop('num_boost_round', 1000),
                    valid_sets=[train_data, val_data],
                    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
                )

                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state),
            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        # æ·»åŠ å›ºå®šå‚æ•°
        best_params['objective'] = 'regression'
        best_params['metric'] = 'rmse'
        best_params['verbose'] = -1

        print(f"\nLightGBMå‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['lightgbm'] = best_params
        self.optimization_results['lightgbm'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_xgboost(self, features_df, target_col='é”€å”®æ•°é‡',
                         n_trials=50, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–XGBoostå‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'max_depth': trial.suggest_int('max_depth', 3, 15),
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 50, 1000),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'random_state': random_state,
                'n_jobs': -1
            }

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model = xgb.XGBRegressor(**params)
                model.fit(
                    X_train_fold, y_train_fold,
                    eval_set=[(X_val_fold, y_val_fold)],
                    verbose=False,
                    early_stopping_rounds=50
                )

                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state),
            pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\nXGBoostå‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['xgboost'] = best_params
        self.optimization_results['xgboost'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_random_forest(self, features_df, target_col='é”€å”®æ•°é‡',
                               n_trials=30, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–éšæœºæ£®æ—å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 5, 30),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),
                'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),
                'random_state': random_state,
                'n_jobs': -1
            }

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model = RandomForestRegressor(**params)
                model.fit(X_train_fold, y_train_fold)

                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\néšæœºæ£®æ—å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['random_forest'] = best_params
        self.optimization_results['random_forest'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_gradient_boosting(self, features_df, target_col='é”€å”®æ•°é‡',
                                   n_trials=30, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–æ¢¯åº¦æå‡å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),
                'random_state': random_state
            }

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model = GradientBoostingRegressor(**params)
                model.fit(X_train_fold, y_train_fold)

                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\næ¢¯åº¦æå‡å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['gradient_boosting'] = best_params
        self.optimization_results['gradient_boosting'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_linear_regression(self, features_df, target_col='é”€å”®æ•°é‡',
                                   n_trials=20, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–çº¿æ€§å›å½’å‚æ•°ï¼ˆä¸»è¦æ˜¯æ­£åˆ™åŒ–å‚æ•°ï¼‰

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            # çº¿æ€§å›å½’æ²¡æœ‰å¤ªå¤šå¯è°ƒå‚æ•°ï¼Œä¸»è¦æ˜¯æ˜¯å¦æ ‡å‡†åŒ–
            normalize = trial.suggest_categorical('normalize', [True, False])

            # åˆ›å»ºæ¨¡å‹
            model = LinearRegression(normalize=normalize)

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model.fit(X_train_fold, y_train_fold)
                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\nçº¿æ€§å›å½’å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['linear_regression'] = best_params
        self.optimization_results['linear_regression'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_ridge(self, features_df, target_col='é”€å”®æ•°é‡',
                       n_trials=30, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–å²­å›å½’å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            # å²­å›å½’çš„ä¸»è¦å‚æ•°
            params = {
                'alpha': trial.suggest_float('alpha', 0.01, 100.0, log=True),
                'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),
                'normalize': trial.suggest_categorical('normalize', [True, False]),
                'solver': trial.suggest_categorical('solver', ['auto', 'svd', 'cholesky',
                                                               'lsqr', 'sparse_cg', 'sag', 'saga']),
                'random_state': random_state
            }

            # åˆ›å»ºæ¨¡å‹
            model = Ridge(**params)

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model.fit(X_train_fold, y_train_fold)
                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\nå²­å›å½’å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['ridge'] = best_params
        self.optimization_results['ridge'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_lasso(self, features_df, target_col='é”€å”®æ•°é‡',
                       n_trials=30, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–Lassoå›å½’å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            # Lassoå›å½’çš„ä¸»è¦å‚æ•°
            params = {
                'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log=True),
                'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]),
                'normalize': trial.suggest_categorical('normalize', [True, False]),
                'selection': trial.suggest_categorical('selection', ['cyclic', 'random']),
                'random_state': random_state
            }

            # åˆ›å»ºæ¨¡å‹
            model = Lasso(**params, max_iter=10000)  # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model.fit(X_train_fold, y_train_fold)
                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\nLassoå›å½’å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['lasso'] = best_params
        self.optimization_results['lasso'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_svr(self, features_df, target_col='é”€å”®æ•°é‡',
                     n_trials=30, cv_folds=3, random_state=42):
        """
        ä¼˜åŒ–æ”¯æŒå‘é‡å›å½’å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆSVRè¾ƒæ…¢ï¼Œå‡å°‘è¯•éªŒæ¬¡æ•°ï¼‰
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°ï¼ˆSVRè¾ƒæ…¢ï¼Œå‡å°‘æŠ˜æ•°ï¼‰
            random_state: éšæœºç§å­

        Returns:
            æœ€ä½³å‚æ•°å’Œæœ€ä½³åˆ†æ•°
        """
        X, y, feature_columns = self.prepare_cv_data(features_df, target_col)

        def objective(trial):
            # SVRçš„ä¸»è¦å‚æ•°
            kernel = trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])

            params = {
                'kernel': kernel,
                'C': trial.suggest_float('C', 0.1, 100.0, log=True),
                'epsilon': trial.suggest_float('epsilon', 0.01, 1.0, log=True),
                'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),
            }

            # å¦‚æœé€‰æ‹©polyæ ¸ï¼Œæ·»åŠ degreeå‚æ•°
            if kernel == 'poly':
                params['degree'] = trial.suggest_int('degree', 2, 5)

            # åˆ›å»ºæ¨¡å‹
            model = SVR(**params)

            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆSVRè¾ƒæ…¢ï¼Œä½¿ç”¨è¾ƒå°‘æŠ˜æ•°ï¼‰
            cv_scores = []

            for train_idx, val_idx in self.time_series_cv_split(X, y, cv_folds):
                X_train_fold = X.iloc[train_idx]
                X_val_fold = X.iloc[val_idx]
                y_train_fold = y.iloc[train_idx]
                y_val_fold = y.iloc[val_idx]

                model.fit(X_train_fold, y_train_fold)
                y_pred = model.predict(X_val_fold)
                score = np.sqrt(mean_squared_error(y_val_fold, y_pred))
                cv_scores.append(score)

            return np.mean(cv_scores)

        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=random_state)
        )

        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        best_params = study.best_params
        best_score = study.best_value

        print(f"\næ”¯æŒå‘é‡å›å½’å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³RMSE: {best_score:.4f}")
        print(f"æœ€ä½³å‚æ•°: {best_params}")

        self.best_params['svr'] = best_params
        self.optimization_results['svr'] = {
            'best_score': best_score,
            'best_params': best_params,
            'study': study
        }

        return best_params, best_score

    def optimize_all_models(self, features_df, target_col='é”€å”®æ•°é‡',
                            n_trials_per_model=30, cv_folds=5, random_state=42):
        """
        ä¼˜åŒ–æ‰€æœ‰æ”¯æŒæ¨¡å‹çš„å‚æ•°

        Args:
            features_df: ç‰¹å¾æ•°æ®
            target_col: ç›®æ ‡åˆ—å
            n_trials_per_model: æ¯ä¸ªæ¨¡å‹çš„è¯•éªŒæ¬¡æ•°
            cv_folds: äº¤å‰éªŒè¯æŠ˜æ•°
            random_state: éšæœºç§å­

        Returns:
            æ‰€æœ‰æ¨¡å‹çš„æœ€ä½³å‚æ•°
        """
        print("å¼€å§‹ä¼˜åŒ–æ‰€æœ‰æ¨¡å‹çš„å‚æ•°...")

        results = {}

        # ä¼˜åŒ–LightGBM
        if 'lightgbm' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–LightGBMå‚æ•° ===")
            try:
                best_params, best_score = self.optimize_lightgbm(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['lightgbm'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"LightGBMä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–XGBoost
        if 'xgboost' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–XGBoostå‚æ•° ===")
            try:
                best_params, best_score = self.optimize_xgboost(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['xgboost'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"XGBoostä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–éšæœºæ£®æ—
        if 'random_forest' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–éšæœºæ£®æ—å‚æ•° ===")
            try:
                best_params, best_score = self.optimize_random_forest(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['random_forest'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"éšæœºæ£®æ—ä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–æ¢¯åº¦æå‡
        if 'gradient_boosting' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–æ¢¯åº¦æå‡å‚æ•° ===")
            try:
                best_params, best_score = self.optimize_gradient_boosting(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['gradient_boosting'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"æ¢¯åº¦æå‡ä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–çº¿æ€§å›å½’
        if 'linear_regression' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–çº¿æ€§å›å½’å‚æ•° ===")
            try:
                # çº¿æ€§å›å½’å‚æ•°è¾ƒå°‘ï¼Œå‡å°‘è¯•éªŒæ¬¡æ•°
                best_params, best_score = self.optimize_linear_regression(
                    features_df, target_col, min(10, n_trials_per_model), cv_folds, random_state
                )
                results['linear_regression'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"çº¿æ€§å›å½’ä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–å²­å›å½’
        if 'ridge' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–å²­å›å½’å‚æ•° ===")
            try:
                best_params, best_score = self.optimize_ridge(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['ridge'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"å²­å›å½’ä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–Lassoå›å½’
        if 'lasso' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–Lassoå›å½’å‚æ•° ===")
            try:
                best_params, best_score = self.optimize_lasso(
                    features_df, target_col, n_trials_per_model, cv_folds, random_state
                )
                results['lasso'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"Lassoå›å½’ä¼˜åŒ–å¤±è´¥: {e}")

        # ä¼˜åŒ–æ”¯æŒå‘é‡å›å½’ï¼ˆSVRè¾ƒæ…¢ï¼Œå‡å°‘è¯•éªŒæ¬¡æ•°ï¼‰
        if 'svr' in self.trainer.supported_algorithms:
            print("\n=== ä¼˜åŒ–æ”¯æŒå‘é‡å›å½’å‚æ•° ===")
            try:
                # SVRè®­ç»ƒè¾ƒæ…¢ï¼Œå‡å°‘è¯•éªŒæ¬¡æ•°å’Œäº¤å‰éªŒè¯æŠ˜æ•°
                best_params, best_score = self.optimize_svr(
                    features_df, target_col,
                    min(15, n_trials_per_model),  # SVRè¾ƒæ…¢ï¼Œå‡å°‘è¯•éªŒæ¬¡æ•°
                    min(3, cv_folds),  # SVRè¾ƒæ…¢ï¼Œå‡å°‘æŠ˜æ•°
                    random_state
                )
                results['svr'] = {'params': best_params, 'score': best_score}
            except Exception as e:
                print(f"æ”¯æŒå‘é‡å›å½’ä¼˜åŒ–å¤±è´¥: {e}")

        print("\næ‰€æœ‰æ¨¡å‹å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        self.print_optimization_summary()

        return results

    def print_optimization_summary(self):
        """æ‰“å°ä¼˜åŒ–ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 50)
        print("å‚æ•°ä¼˜åŒ–ç»“æœæ‘˜è¦")
        print("=" * 50)

        for model_name, result in self.optimization_results.items():
            print(f"\n{model_name.upper()}:")
            print(f"  æœ€ä½³RMSE: {result['best_score']:.4f}")
            print(f"  å‚æ•°æ•°é‡: {len(result['best_params'])}ä¸ª")

        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        if self.optimization_results:
            best_model = min(self.optimization_results.items(),
                             key=lambda x: x[1]['best_score'])[0]
            best_score = self.optimization_results[best_model]['best_score']
            print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model} (RMSE: {best_score:.4f})")

        print("=" * 50)